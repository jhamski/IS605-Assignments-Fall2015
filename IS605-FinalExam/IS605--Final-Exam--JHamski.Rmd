---
title: "IS605--Final-Examâ€”Hamski"
author: "James Hamski"
date: "December 19, 2015"
output: html_document
---
```{r}
require(readr)

```

# Section 1 - Review
1)  
$$
\begin{bmatrix}
    1 & -1 & 3 & 5 \\
    2 & 1 & 5 & -9 \\
    6 & -1 & -2 & 4
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
    -2 & 2 & 6 & -10 \\
    2 & 1 & 5 & -9 \\
    6 & -1 & -2 & 4
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
    1 & -1 & 3 & 5 \\
    0 & 3 & 11 & -19 \\
    6 & -1 & -2 & 4
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
    -6 & 6 & 18 & -30 \\
    2 & 1 & 5 & -9 \\
    6 & -1 & -2 & 4
\end{bmatrix}
\Rightarrow
$$
  
$$
\begin{bmatrix}
    1 & -1 & 3 & 5 \\
    0 & 3 & 11 & -19 \\
    0 & 5 & 16 & -26
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
    1 & -1 & 3 & 5 \\
    0 & 3 & 11 & -19 \\
    0 & 5 & 16 & -26
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
    1 & -1 & 3 & 5 \\
    0 & -15 & -55 & 95 \\
    0 & 15 & 48 & -78
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
    1 & -1 & 3 & 5 \\
    0 & 3 & 11 & -19 \\
    0 & 0 & -7 & 17
\end{bmatrix}
$$
matrix rank = 3

$$
A =
\begin{bmatrix}
    1 & -1 & 3 & 5 \\
    2 & 1 & 5 & -9 \\
    6 & -1 & -2 & 4
\end{bmatrix}
$$

2)  
$$
t(A) = 
\begin{bmatrix}
    1 & 2 & 6 \\
    -1 & 1 & -1 \\
    3 & 5 & -2 \\
    5 & -9 & 4
\end{bmatrix}
$$


3)
  
First, understand what a basis is. A basis is a set of vectors that allows the expression of any vector in the vector space through linear combination, and are linearly independent. My  mental model for this is to think of an arm painting a wall ($R^2$) that can move along the X and Y axes - combining different magnitudes in X and Y allows you to paint the entire wall (i.e. create any vector in the vector space).

An orthonormal basis is a basis by the definition above, with the additional constraint that they are normal unit length (i.e. 1) and orthoganal - meaning their inner product is 0 (they are all perpendicular to eachother). 

The orthonormal basis of $R^3$ are (0,0,1), (0,1,0), and (1,0,0). These are mutually perpendicular, normal, and may be combined to create any vector in $R^3$. 

4)  
$$ t^3-6t^2+6t-17 $$

5)
```{r}
eigenvalues <- polyroot(c(-17, 6, -6, 1))
print(eigenvalues)
```

```{r}
a <- matrix(c(5,0,3,0,1,-2,1,2,0), nrow=3, byrow=T)
e.mat <- matrix(c(eigenvalues[1], 0, 0, 0, eigenvalues[1], 0, 0, 0,eigenvalues[1]), nrow=3, byrow=T)
Eig.Space <- (e.mat - a)


```



7)  *Assuming that we are repeatedly sampling sets of numbers (each set is of size n) from an unknown probability density function. What can we say about the average value of each set?*

According to the Central Limit Theorem, we can say that these averages will be normal (Gaussian) distributed



```{r}

```

# Section 2 - Mini-coding Assignments
2.1)  
```{r}
#First, directly calculate the probability mass function:
binom <- function(int){
  prob<-choose(20, int)*(0.25^int)*(0.75^(20-int))
  return(prob)}

integers <- seq(from=0, to=20, by=1)

binom.freq <- sapply(integers, FUN=binom)
plot(binom.freq, type="s")
```

```{r}
iterations <- 1000
#sample from the probabilities specified by the PMF

PMF.simulation <- sample(x = integers, size=iterations, replace = T, prob = binom.freq)
hist(PMF.simulation)
```

```{r}
simulated.prob<-table(PMF.simulation)/iterations
print(simulated.prob)
```

```{r}
#note indexing in table and direct calculation with binom function means 6 vs 5 here, X==5 is being calculated either way
all.equal(simulated.prob[6], binom(5))
```

Higher values for X==k are so improbable they do not turn up in the sampling of 1000 numbers, i.e.:
```{r}
binom(20)
```

2.2)  
```{r}
#note I opted to open the data in Excel and save with commas as delineators. 
col.names = c("displacement", "horsepower", "weight", "acceleration", "mpg")
auto <- read_csv(file="auto-mpg.csv", col_names=col.names)

#From the help file: Performs a principal components analysis on the given data matrix and returns the results as an object of class prcomp...The calculation is done by a singular value decomposition of the (centered and possibly scaled) data matrix, not by using eigen on the covariance matrix. This is generally the preferred method for numerical accuracy. 
pc <- prcomp(auto, center=T)
print(pc)

plot(x=auto$mpg, y=auto$displacement)

#auto %*% pc$rotation
```


```{r}
auto.svd <- svd(auto)

```

```{r}
# perform principal components analysis
pca <- prcomp(auto) 

# project new data onto the PCA space
scale(auto, pca$center, pca$scale) %*% pca$rotation 

```



2.3)  
Probability of drawing an item from uniform distribution: $\frac{1}{n}$  
Prob. of not drawing an item: $1-\frac{1}{n}$  
Prob of not drawing for sample size n: $(1-\frac{1}{n})^n$  
The probability of an item not being chosen:  
$$\lim_{x\to\infty} (1-\frac{1}{n})^n =$$

$$y=(1-\frac{1}{n})^n$$
$$ln(y)=ln(1-\frac{1}{n})^n$$
$$ln(y)=n*ln(1-\frac{1}{n})$$
$$ln(y)=-n(\frac{1}{n}+\frac{1/n^2}{2}+\frac{1/n^3}{3}+\cdot\cdot\cdot)$$
$$ln(y)=-1-\frac{1/n}{2}-\frac{1/n^2}{3}-\cdot\cdot\cdot$$
$$y=e^{-1-\frac{1/n}{2}-\frac{1/n^2}{3}-\cdot\cdot\cdot}$$
$$y=e^{-1}\cdot e^{\frac{1/n}{2}-\frac{1/n^2}{3}-\cdot\cdot\cdot}$$
$$\lim_{x\to\infty} (1-\frac{1}{n})^n = e^{-1}\cdot \lim_{x\to\infty} e^{\frac{1/n}{2}-\frac{1/n^2}{3}-\cdot\cdot\cdot}$$
$$= e^{-1}\cdot e^{0}$$
$$= \frac{1}{e}$$
$$\approx 0.368$$
Probability of an item being chosen:
$$1-0.368 \approx 0.632$$


```{r}
n = 100
toy.data <- rnorm(n, 0)

repeats<-NULL
iterations <- 10000

for(i in 1:iterations){
  bootstrap <-sample(toy.data, size=n, replace = T)
  common <-length(intersect(bootstrap,toy.data))
  repeats <- c(repeats, common)}

mean(repeats)
```



# Section 3 - Mini-project 

```{r}
x <- read.table('ex3x.dat')
y <- read.table('ex3y.dat')
data <- cbind(x, y)
colnames(data)<-c("SqFt", "Rooms","Price")

sd.SqFt <-sd(data$SqFt)
sd.Rooms <-sd(data$Rooms)
sd.Price <-sd(data$Price)

mean.SqFt <-mean(data$SqFt)
mean.Rooms <-mean(data$Rooms)
mean.Price <-mean(data$Price)

standardize <- function(x, mean, sd){return((x-mean)/sd)}

stand.data.SqFt <- sapply(data$SqFt, FUN=standardize, mean.SqFt, sd.SqFt)
stand.data.Rooms <- sapply(data$Rooms, FUN=standardize, mean.Rooms, sd.Rooms)
stand.data.Price <- sapply(data$Price, FUN=standardize, mean.Price, sd.Price)

stand.data <- as.data.frame(cbind(stand.data.SqFt, stand.data.Rooms, stand.data.Price))
colnames(stand.data)<-c("SqFt", "Rooms","Price")
```

```{r}

x <- stand.data$SqFt
y <- stand.data$Price
# squared error cost function
cost <- function(X, y, theta) {
  sum( (X %*% theta - y)^2 ) / (2*length(y))
}

# learning rate and iteration limit
alpha <- 0.01
num_iters <- 1000

# keep history
cost_history <- double(num_iters)
theta_history <- list(num_iters)

# initialize coefficients
theta <- matrix(c(0,0), nrow=2)

# add a column of 1's for the intercept coefficient
X <- cbind(1, matrix(x))

# gradient descent
for (i in 1:num_iters) {
  error <- (X %*% theta - y)
  delta <- t(X) %*% error / length(y)
  theta <- theta - alpha * delta
  cost_history[i] <- cost(X, y, theta)
  theta_history[[i]] <- theta
}

print(theta)

# plot data and converging fit
plot(x,y, col=rgb(0.2,0.4,0.6,0.4), main='Linear regression by gradient descent')
for (i in c(1,3,6,10,14,seq(20,num_iters,by=10))) {
  abline(coef=theta_history[[i]], col=rgb(0.8,0,0,0.3))
}
abline(coef=theta, col='blue')
```

```{r}
plot(cost_history, type='line', col='blue', lwd=2, main='Cost function', ylab='cost', xlab='Iterations')
```


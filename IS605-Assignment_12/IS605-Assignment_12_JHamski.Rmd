---
title: "IS605-Assignment-12"
author: "J. Hamski"
date: "November 14, 2015"
output: html_document
---

##BIAS VARIANCE TRADEOFF IN R

```{r, message=FALSE, warning=FALSE}
require(stats)
require(boot)
require(readr)
```


```{r}
col.names = c("displacement", "horsepower", "weight", "acceleration", "mpg")
cars <- read_csv(file="auto-mpg.csv", col_names=col.names)
```

```{r}
glm.fit.1 <- glm(mpg~poly(displacement+horsepower+weight+acceleration,1), data=cars)
glm.fit.2 <- glm(mpg~poly(displacement+horsepower+weight+acceleration,2), data=cars)
glm.fit.3 <- glm(mpg~poly(displacement+horsepower+weight+acceleration,3), data=cars)
glm.fit.4 <- glm(mpg~poly(displacement+horsepower+weight+acceleration,4), data=cars)
glm.fit.5 <- glm(mpg~poly(displacement+horsepower+weight+acceleration,5), data=cars)
glm.fit.6 <- glm(mpg~poly(displacement+horsepower+weight+acceleration,6), data=cars)
glm.fit.7 <- glm(mpg~poly(displacement+horsepower+weight+acceleration,7), data=cars)
glm.fit.8 <- glm(mpg~poly(displacement+horsepower+weight+acceleration,8), data=cars)
glm.fit.9 <- glm(mpg~poly(displacement+horsepower+weight+acceleration,9), data=cars)
glm.fit.10 <- glm(mpg~poly(displacement+horsepower+weight+acceleration,10), data=cars)
```


```{r}
set.seed(284)
cv.err.1 <- cv.glm(cars,glm.fit.1,K=5)$delta[1]
cv.err.2 <- cv.glm(cars,glm.fit.2,K=5)$delta[1]
cv.err.3 <- cv.glm(cars,glm.fit.3,K=5)$delta[1]
cv.err.4 <- cv.glm(cars,glm.fit.4,K=5)$delta[1]
cv.err.5 <- cv.glm(cars,glm.fit.5,K=5)$delta[1]
cv.err.6 <- cv.glm(cars,glm.fit.6,K=5)$delta[1]
cv.err.7 <- cv.glm(cars,glm.fit.7,K=5)$delta[1]
cv.err.8 <- cv.glm(cars,glm.fit.8,K=5)$delta[1]
cv.err.9 <- cv.glm(cars,glm.fit.9,K=5)$delta[1]
cv.err.10 <- cv.glm(cars,glm.fit.10,K=5)$delta[1]


cv.err <- rbind(cv.err.1, cv.err.2,cv.err.3,cv.err.4,cv.err.5,cv.err.6,cv.err.7,cv.err.8, cv.err.9, cv.err.10)
```

```{r}
plot(x = 1:10, cv.err, type='b')
```

A better implementation:

```{r}
fit.cv <-function(degree){
  
  glm.fit <- glm(mpg~poly(displacement+horsepower+weight+acceleration,degree), data=cars)
  cv.error <- cv.glm(cars, glm.fit, K=5)$delta[1]
  return(cv.error)}

max.degree = 10

glm.fit.all <- mapply(1:max.degree, FUN=fit.cv)
plot(glm.fit.all, type='b')
```

I noticed there is a large variation in the prediction errors depending on the random sampling that happens during cross-validation. I had to try a few values for set.seed before it showed the expected U-shape. Therefore, it makes sense to run many iterations of the cross-validation process and view the results. This confirms that the bias-variance tradeoff is observed in the prediction errors resulting from polynomial models of varying degrees. 
```{r}
get.cv.errors <- function(){mapply(1:max.degree, FUN=fit.cv)}

# note this is a bit slow
a <- replicate(50, exp=get.cv.errors())

boxplot.matrix(a, use.cols=FALSE)
```


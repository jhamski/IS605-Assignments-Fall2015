---
title: "IS605--Final-Examâ€”Hamski"
author: "James Hamski"
date: "December 19, 2015"
output: html_document
---
```{r}
require(readr)

```

# Section 1 - Review
1)  
$$
\begin{bmatrix}
    1 & -1 & 3 & 5 \\
    2 & 1 & 5 & -9 \\
    6 & -1 & -2 & 4
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
    -2 & 2 & 6 & -10 \\
    2 & 1 & 5 & -9 \\
    6 & -1 & -2 & 4
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
    1 & -1 & 3 & 5 \\
    0 & 3 & 11 & -19 \\
    6 & -1 & -2 & 4
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
    -6 & 6 & 18 & -30 \\
    2 & 1 & 5 & -9 \\
    6 & -1 & -2 & 4
\end{bmatrix}
\Rightarrow
$$
  
$$
\begin{bmatrix}
    1 & -1 & 3 & 5 \\
    0 & 3 & 11 & -19 \\
    0 & 5 & 16 & -26
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
    1 & -1 & 3 & 5 \\
    0 & 3 & 11 & -19 \\
    0 & 5 & 16 & -26
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
    1 & -1 & 3 & 5 \\
    0 & -15 & -55 & 95 \\
    0 & 15 & 48 & -78
\end{bmatrix}
\Rightarrow
\begin{bmatrix}
    1 & -1 & 3 & 5 \\
    0 & 3 & 11 & -19 \\
    0 & 0 & -7 & 17
\end{bmatrix}
$$
matrix rank = 3

$$
A =
\begin{bmatrix}
    1 & -1 & 3 & 5 \\
    2 & 1 & 5 & -9 \\
    6 & -1 & -2 & 4
\end{bmatrix}
$$

2)  
$$
t(A) = 
\begin{bmatrix}
    1 & 2 & 6 \\
    -1 & 1 & -1 \\
    3 & 5 & -2 \\
    5 & -9 & 4
\end{bmatrix}
$$


3)
  
First, understand what a basis is. A basis is a set of vectors that allows the expression of any vector in the vector space through linear combination, and are linearly independent. My  mental model for this is to think of an arm painting a wall ($R^2$) that can move along the X and Y axes - combining different magnitudes in X and Y allows you to paint the entire wall (i.e. create any vector in the vector space).

An orthonormal basis is a basis by the definition above, with the additional constraint that they are normal unit length (i.e. 1) and orthoganal - meaning their inner product is 0 (they are all perpendicular to eachother). 

The orthonormal basis of $R^3$ are (0,0,1), (0,1,0), and (1,0,0). These are mutually perpendicular, normal, and may be combined to create any vector in $R^3$. 

4)  
$$ t^3-6t^2+6t-17 $$

5)
```{r}
eigenvalues <- polyroot(c(-17, 6, -6, 1))
print(eigenvalues)
```

```{r}
a <- matrix(c(5,0,3,0,1,-2,1,2,0), nrow=3, byrow=T)
e.mat <- matrix(c(eigenvalues[1], 0, 0, 0, eigenvalues[1], 0, 0, 0,eigenvalues[1]), nrow=3, byrow=T)
Eig.Space <- (e.mat - a)


```



7)  *Assuming that we are repeatedly sampling sets of numbers (each set is of size n) from an unknown probability density function. What can we say about the average value of each set?*

According to the Central Limit Theorem, we can say that these averages will be normal (Gaussian) distributed



```{r}

```

# Section 2 - Mini-coding Assignments
2.1)  
```{r}
binom <- function(int){
  prob<-choose(20, int)*(0.25^int)*(0.75^(20-int))
  return(prob)
}
integers <- seq(from=0, to=20, by=1)

binom.freq <- sapply(integers, FUN=binom)
plot(binom.freq)
```

2.2)  
```{r}
#note I opted to open the data in Excel and save with commas as delineators. 
col.names = c("displacement", "horsepower", "weight", "acceleration", "mpg")
auto <- read_csv(file="auto-mpg.csv", col_names=col.names)

#From the help file: Performs a principal components analysis on the given data matrix and returns the results as an object of class prcomp...The calculation is done by a singular value decomposition of the (centered and possibly scaled) data matrix, not by using eigen on the covariance matrix. This is generally the preferred method for numerical accuracy. 
pc <- prcomp(auto, center=T)
print(pc)
```


2.3)  
```{r}
n = 100
toy.data <- rnorm(n, 0)

bootstrap <- replicate(100, expr=sample(toy.data, size=n, replace = T))



for(i in 1:100){
  bootstrap <-sample(toy.data, size=n, replace = T)
  intersect <-length(intersect(bootstrap,toy.data))
  print(intersect)
}

```



# Section 3 - Mini-project 

```{r}
x <- read.table('ex3x.dat')
y <- read.table('ex3y.dat')
data <- cbind(x, y)
colnames(data)<-c("SqFt", "Rooms","Price")

sd.SqFt <-sd(data$SqFt)
sd.Rooms <-sd(data$Rooms)
sd.Price <-sd(data$Price)

mean.SqFt <-mean(data$SqFt)
mean.Rooms <-mean(data$Rooms)
mean.Price <-mean(data$Price)

standardize <- function(x, mean, sd){return((x-mean)/sd)}

stand.data.SqFt <- sapply(data$SqFt, FUN=standardize, mean.SqFt, sd.SqFt)
stand.data.Rooms <- sapply(data$Rooms, FUN=standardize, mean.Rooms, sd.Rooms)
stand.data.Price <- sapply(data$Price, FUN=standardize, mean.Price, sd.Price)

stand.data <- as.data.frame(cbind(stand.data.SqFt, stand.data.Rooms, stand.data.Price))
colnames(stand.data)<-c("SqFt", "Rooms","Price")
```

```{r}

x <- stand.data$SqFt
y <- stand.data$Price
# squared error cost function
cost <- function(X, y, theta) {
  sum( (X %*% theta - y)^2 ) / (2*length(y))
}

# learning rate and iteration limit
alpha <- 0.01
num_iters <- 1000

# keep history
cost_history <- double(num_iters)
theta_history <- list(num_iters)

# initialize coefficients
theta <- matrix(c(0,0), nrow=2)

# add a column of 1's for the intercept coefficient
X <- cbind(1, matrix(x))

# gradient descent
for (i in 1:num_iters) {
  error <- (X %*% theta - y)
  delta <- t(X) %*% error / length(y)
  theta <- theta - alpha * delta
  cost_history[i] <- cost(X, y, theta)
  theta_history[[i]] <- theta
}

print(theta)

# plot data and converging fit
plot(x,y, col=rgb(0.2,0.4,0.6,0.4), main='Linear regression by gradient descent')
for (i in c(1,3,6,10,14,seq(20,num_iters,by=10))) {
  abline(coef=theta_history[[i]], col=rgb(0.8,0,0,0.3))
}
abline(coef=theta, col='blue')
```

```{r}
plot(cost_history, type='line', col='blue', lwd=2, main='Cost function', ylab='cost', xlab='Iterations')
```

